/* This Source Code Form is subject to the terms of the Mozilla Public
 * License, v. 2.0. If a copy of the MPL was not distributed with this
 * file, You can obtain one at http://mozilla.org/MPL/2.0/. */


/*
Rounds a timestamp up to the next interval
 */
CREATE OR REPLACE FUNCTION date_trunc_up(interval_precision TEXT, ts TIMESTAMP)
	RETURNS TIMESTAMP LANGUAGE SQL
IMMUTABLE
AS $$
SELECT CASE
			 WHEN ts = date_trunc(interval_precision, ts) THEN ts
			 ELSE date_trunc(interval_precision, ts + ('1 ' || interval_precision)::INTERVAL)
			 END
$$;

/*
	The following views are all generated by generateCompressedReadingViews.js in this folder.
	This is necessary because they can't be wrapped in a function (otherwise predicates would not be pushed down).
*/

/*
The query shared by all of these views gets slow when one of two things happen:
	1) It has to scan a large percentage of the readings table
	2) It has to generate a large number of rows (by compressing to a small interval)
We pick the best of both worlds by only materializing the large duration tables (day+).
These produce far fewer rows than the non-materialized tables, making them cheap to store,
but they benefit from materialization because they require a scan of a large percentage of
the readings table (to aggregate data over a large time range).

The small tables, on the other hand, are only queried for small time ranges that produce a few hundred data points.
This is the best-case scenario for these queries, and leads to quick execution times if the table is properly clustered.
 */
CREATE VIEW
minutely_readings
	AS SELECT
				r.meter_id AS meter_id,
				-- This gives the weighted average of the reading rates, defined as
				-- sum(reading_rate * overlap_duration) / sum(overlap_duration)
				(sum(
						 (r.reading / (extract(EPOCH FROM (r.end_timestamp - r.start_timestamp)) / 3600)) -- Reading rate in kw
						 *
						 extract(EPOCH FROM -- The number of seconds that the reading shares with the interval
										 least(r.end_timestamp, gen.interval_start + '1 hour'::INTERVAL)
										 -
										 greatest(r.start_timestamp, gen.interval_start)
						 )
				 ) / sum(
						 extract(EPOCH FROM -- The number of seconds that the reading shares with the interval
										 least(r.end_timestamp, gen.interval_start + '1 hour'::INTERVAL)
										 -
										 greatest(r.start_timestamp, gen.interval_start)
						 )
				 )) AS reading_rate,
				tsrange(gen.interval_start, gen.interval_start + '1 hour'::INTERVAL, '()') AS time_interval
			FROM readings r
				CROSS JOIN LATERAL generate_series(
						date_trunc('hour', r.start_timestamp),
						-- Subtract 1 interval width because generate_series is end-inclusive
						date_trunc_up('hour', r.end_timestamp) - '1 hour'::INTERVAL,
						'1 hour'::INTERVAL
				) gen(interval_start)
			GROUP BY r.meter_id, gen.interval_start;

CREATE VIEW
hourly_readings
	AS SELECT
				r.meter_id AS meter_id,
				-- This gives the weighted average of the reading rates, defined as
				-- sum(reading_rate * overlap_duration) / sum(overlap_duration)
				(sum(
						 (r.reading / (extract(EPOCH FROM (r.end_timestamp - r.start_timestamp)) / 3600)) -- Reading rate in kw
						 *
						 extract(EPOCH FROM -- The number of seconds that the reading shares with the interval
										 least(r.end_timestamp, gen.interval_start + '1 hour'::INTERVAL)
										 -
										 greatest(r.start_timestamp, gen.interval_start)
						 )
				 ) / sum(
						 extract(EPOCH FROM -- The number of seconds that the reading shares with the interval
										 least(r.end_timestamp, gen.interval_start + '1 hour'::INTERVAL)
										 -
										 greatest(r.start_timestamp, gen.interval_start)
						 )
				 )) AS reading_rate,
				tsrange(gen.interval_start, gen.interval_start + '1 hour'::INTERVAL, '()') AS time_interval
			FROM readings r
				CROSS JOIN LATERAL generate_series(
						date_trunc('hour', r.start_timestamp),
						-- Subtract 1 interval width because generate_series is end-inclusive
						date_trunc_up('hour', r.end_timestamp) - '1 hour'::INTERVAL,
						'1 hour'::INTERVAL
				) gen(interval_start)
			GROUP BY r.meter_id, gen.interval_start;

CREATE MATERIALIZED VIEW
daily_readings
	AS SELECT
				r.meter_id AS meter_id,
				-- This gives the weighted average of the reading rates, defined as
				-- sum(reading_rate * overlap_duration) / sum(overlap_duration)
				(sum(
						 (r.reading / (extract(EPOCH FROM (r.end_timestamp - r.start_timestamp)) / 3600)) -- Reading rate in kw
						 *
						 extract(EPOCH FROM -- The number of seconds that the reading shares with the interval
										 least(r.end_timestamp, gen.interval_start + '1 day'::INTERVAL)
										 -
										 greatest(r.start_timestamp, gen.interval_start)
						 )
				 ) / sum(
						 extract(EPOCH FROM -- The number of seconds that the reading shares with the interval
										 least(r.end_timestamp, gen.interval_start + '1 day'::INTERVAL)
										 -
										 greatest(r.start_timestamp, gen.interval_start)
						 )
				 )) AS reading_rate,
				tsrange(gen.interval_start, gen.interval_start + '1 day'::INTERVAL, '()') AS time_interval
			FROM readings r
				CROSS JOIN LATERAL generate_series(
						date_trunc('day', r.start_timestamp),
						-- Subtract 1 interval width because generate_series is end-inclusive
						date_trunc_up('day', r.end_timestamp) - '1 day'::INTERVAL,
						'1 day'::INTERVAL
				) gen(interval_start)
			GROUP BY r.meter_id, gen.interval_start
		 -- The order by ensures that the materialized view will be clustered in this way.
			ORDER BY gen.interval_start, r.meter_id;

-- We need a gist index to support the @> operation.
CREATE INDEX idx_daily_readings ON daily_readings USING GIST(time_interval, meter_id);


CREATE FUNCTION refresh_daily_readings()
	AS $$
		-- TODO: Investigate the performance impacts of CONCURRENTLY and dropping / recreating the index
		REFRESH MATERIALIZED VIEW daily_readings;
	$$ LANGUAGE 'plpgsql';


/*
The following function determines the correct duration view to query from, and returns compressed data from it.
 */
CREATE FUNCTION compressed_readings_2(meter_ids INTEGER[], start_timestamp TIMESTAMP, end_timestamp TIMESTAMP)
	RETURNS TABLE(meter_id INTEGER, reading_rate REAL, time_interval TSRANGE)
AS $$
DECLARE
	requested_interval INTERVAL;
	requested_range TSRANGE;
	minimum_num_pts INTEGER = 50;
BEGIN
	/*
		We need to figure out which table to query from.
		We choose that we want at least 50 points for a given time interval because a much larger
		number would lead to really large scans of the hourly table (which is not materialized) right
		before the switch to days.

		For example, if we specified at least 100 points, and the range was 99 days and 23 hours, then we would have
		to scan (100 days * 24 hours - 1 hour) * (60 minutes) * (n meters) worth of data. That's 143940 rows per meter
		if the meters are minute precision! Each meter reading has at least 4 ints (meter_id, reading, start/end timestamp),
		totalling slightly over 2.2 megabytes of data per meter. This balloons significantly when it's sent as JSON,
		leading to unacceptable load times for the client, especially if they're on a mobile device.
	*/

	-- TODO: Shrink this to actual timestamps that exist.
	requested_interval := end_timestamp - start_timestamp;
	requested_range := tsrange(start_timestamp, end_timestamp, '[]');

	IF extract(DAY FROM requested_interval) >= minimum_num_pts THEN
		RETURN QUERY
			SELECT *
			FROM daily_readings
			INNER JOIN unnest(meter_ids) meters(id) ON daily_readings.meter_id = meters.id
			WHERE requested_range @> time_interval;
	ELSIF extract(HOURS FROM requested_interval) >= minimum_num_pts THEN
		RETURN QUERY
		SELECT *
		FROM hourly_readings
			INNER JOIN unnest(meter_ids) meters(id) ON hourly_readings.meter_id = meters.id
		WHERE requested_range @> time_interval;
	ELSE
		RETURN QUERY
		SELECT *
		FROM minutely_readings
			INNER JOIN unnest(meter_ids) meters(id) ON minutely_readings.meter_id = meters.id
		WHERE requested_range @> time_interval;
	END IF;
END
$$ LANGUAGE 'plpgsql';
